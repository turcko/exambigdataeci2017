DEVELOPMENT:

Using mapreduce with PYTHON (need Hadoop Streaming API)
-------------------------------------------------------

//Making executable Python's files.
//The codes of .py files are outhere.

> sudo chmod +x mapper.py
> sudo chmod +x reducer.py


//Checking HDFS root content
> hadoop dfs -ls /


//Including books in Hadoop file system.
//Previously, I downloaded some books (3) in PLAIN TEXT UTF-8 format.

> hadoop dfs -put /home/hadoop/books/ /books


//Using Hadoop Streaming API
//It's nedd Hadoop Streaming API for pass data between Map and Reduce 
//code via STDIN and STDOUT. I will simply use Pythonâ€™s sys.stdin to 
//read input data and print our own output to sys.stdout.

> hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-*streaming*.jar \
-mapper /home/uhadoop/mapper.py -reducer /home/uhadoop/reducer.py -input \
/books/* -output /books_output

----------------------------PROCESS OUTPUT----------------------------
...
17/08/09 09:11:08 INFO mapreduce.Job:  map 0% reduce 0%
17/08/09 09:11:17 INFO mapreduce.Job:  map 67% reduce 0%
17/08/09 09:11:18 INFO mapreduce.Job:  map 100% reduce 0%
17/08/09 09:11:25 INFO mapreduce.Job:  map 100% reduce 100%
17/08/09 09:11:27 INFO mapreduce.Job: Job job_1502278054204_0005 completed successfully
17/08/09 09:11:27 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=6091227
		FILE: Number of bytes written=12679311
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=3684751
		HDFS: Number of bytes written=879066
		HDFS: Number of read operations=12
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=3
		Launched reduce tasks=1
		Data-local map tasks=3
		Total time spent by all maps in occupied slots (ms)=23090
		Total time spent by all reduces in occupied slots (ms)=5147
		Total time spent by all map tasks (ms)=23090
		Total time spent by all reduce tasks (ms)=5147
		Total vcore-milliseconds taken by all map tasks=23090
		Total vcore-milliseconds taken by all reduce tasks=5147
		Total megabyte-milliseconds taken by all map tasks=23644160
		Total megabyte-milliseconds taken by all reduce tasks=5270528
	Map-Reduce Framework
		Map input records=78097
		Map output records=629893
		Map output bytes=4831435
		Map output materialized bytes=6091239
		Input split bytes=269
		Combine input records=0
		Combine output records=0
		Reduce input groups=81941
		Reduce shuffle bytes=6091239
		Reduce input records=629893
		Reduce output records=81941
		Spilled Records=1259786
		Shuffled Maps =3
		Failed Shuffles=0
		Merged Map outputs=3
		GC time elapsed (ms)=524
		CPU time spent (ms)=11500
		Physical memory (bytes) snapshot=983683072
		Virtual memory (bytes) snapshot=7687897088
		Total committed heap usage (bytes)=719323136
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=3684482
	File Output Format Counters 
		Bytes Written=879066
17/08/09 09:11:27 INFO streaming.StreamJob: Output directory: /books_output
----------------------------PROCESS OUTPUT----------------------------


//Checking the results in HDFS

> hadoop dfs -ls /books_output

----------------------------PROCESS OUTPUT----------------------------

Found 2 items
-rw-r--r--   1 uhadoop supergroup          0 2017-08-09 09:11 /books_output/_SUCCESS
-rw-r--r--   1 uhadoop supergroup     879066 2017-08-09 09:11 /books_output/part-00000

----------------------------PROCESS OUTPUT----------------------------


//Showing the results

> hadoop dfs -cat /books_output/part-00000

----------------------------PROCESS OUTPUT----------------------------
...

"Outline        2
"PROJECT        3
"Padova 1
"Per    1
"Perdimenti"    1
"Perspective    1
"Perspective")  1
"Philosophical  1
"Phoenix"       1
"Plain  2
"Poor   1
"Pre-Dravidians."       1
"Project        7
"Project").     1
"Prospettiva    2
"Prospettiva"   2
"Punics,"       1
"Pyramid        2

...
----------------------------PROCESS OUTPUT----------------------------


